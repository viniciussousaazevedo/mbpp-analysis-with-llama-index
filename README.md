# Mbpp Script for LLM analysis

## O que foi feito?
Basicamente, utilizei o dataset do [mbpp](https://huggingface.co/datasets/google-research-datasets/mbpp), que cont√©m descri√ß√µes de problemas de programa√ß√£o juntamente com solu√ß√£o e testes da solu√ß√£o, para observar como o Llama 3.1 70B cria sua pr√≥pria su√≠te de testes por meio de uma integra√ß√£o com RAG simples. O que fiz foi criar uma c√≥pia de cada problema removendo a su√≠te de testes e solicitando para a LLM criar a sua pr√≥pria. Uma vez isso feito, comparei o qu√£o bem sucedida a LLM se saiu principalmente em compara√ß√£o com os testes do pr√≥prio dataset que apontam sucesso nos seus c√≥digos de resolu√ß√£o de problemas.

## Como rodar?
Para rodar o script, basta executar esse comando `pip` para instalar as depend√™ncias:
```bash
pip install llama-index-core llama-index-llms-ollama llama-index-embeddings-huggingface llama-index-llms-groq llama-index-readers-json
```
Uma vez instalado, basta executar no root do projeto o comando: 
```bash
python ./mbpp_script.py
```
Os resultados estar√£o presentes nos arquivos locais `error_logs.txt` e `prompt history.txt`

## Interpreta√ß√µes de "erros na gera√ß√£o de teste"
- Em momentos de teste e an√°lise do conte√∫do "errado" da LLM, encontrei p√©rolas como `ERROR IN LLM TEST EXECUTION: assert is_not_prime(1) == True (question id: 3)`ü§¶‚Äç‚ôÇÔ∏è
- De quem √© a culpa? Bem, no fim das contas, devemos analisar tudo como "pode haver problema com":
  - LLM
  - C√≥digo da solu√ß√£o
  - Descri√ß√µes de problemas (potenciais "US's")
  - Prompt Engineer
  - Eu mesmo que posso ter feito o script com algum defeito rs (ningu√©m √© perfeito)
- Gostaria de ter removido todos os outros poss√≠veis causadores de problema da jogada para analisar apenas o desempenho da LLM, mas √© um dataset imenso para an√°lise manual...
- Acredito que uma boa pr√°tica √© analisar sempre os logs e o contexto antes de apontar o dedo para um desses cinco
- O arquivo `error_logs.txt` citado anteriormente guarda todos os erros logados do script

## Melhorias em prompt
- o arquivo `prompt_history.txt`, citado anteriormente, guarda todo o hist√≥rico de prompts que utilizei quando o script estava finalizado, juntamente com alguns dados estat√≠sticos referentes a aquela execu√ß√£o em espec√≠fico. De modo geral, houve uma melhora significativa com adi√ß√£o de elementos de engenharia de prompt, como few-shot prompting, esclarecimento de output e afins. Quest√µes mais complexas foram mais dif√≠ceis de fazer a LLM passar, naturalmente.

## Resultados finais
Foram gerados X casos de teste para Y problemas. De modo geral, os resultados obtidos com LLM foram:
- Testes compilados com sucesso: %
- Testes que aprovaram o c√≥digo gerado: %
- Problemas que passaram em todos os testes: %
> P.S.: N√£o houveram erros em testes do pr√≥prio dataset

## Conclus√µes: Como isso pode contribuir para nosso projeto?
Brincar com esse dataset me ajudou a ter um pouco mais de no√ß√£o em uso e consumo de RAG, que muito provavelmente vamos usar no contexto do projeto, vale a pena com certeza dar uma olhada... Al√©m disso, esses resultados me mostraram um pouco mais sobre a import√¢ncia de prompt engineering e que nem sempre o culpado vai ser a LLM que fez seu trabalho muitas vezes at√© bem feito!

## Contribuindo com o script...
Minhas maiores indaga√ß√µes em como melhorar esse script consistem em:
- Adaptar o c√≥digo para utilizar apenas a descri√ß√£o (acredito ser o mais urgente, por hora)
- Como identificar falsos positivos e falsos negativos no script?
- Como acertar qual √© o culpado do teste falho de forma automatizada?